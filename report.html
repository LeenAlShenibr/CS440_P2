<html><head><title>AI P2</title><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol.lst-kix_3bj6e1vx2xo8-1.start{counter-reset:lst-ctn-kix_3bj6e1vx2xo8-1 0}.lst-kix_3bj6e1vx2xo8-0>li:before{content:"" counter(lst-ctn-kix_3bj6e1vx2xo8-0,decimal) ". "}.lst-kix_3bj6e1vx2xo8-1>li{counter-increment:lst-ctn-kix_3bj6e1vx2xo8-1}.lst-kix_3bj6e1vx2xo8-1>li:before{content:"" counter(lst-ctn-kix_3bj6e1vx2xo8-1,lower-latin) ". "}.lst-kix_ebc7j21k1a4o-6>li:before{content:"\0025cf  "}.lst-kix_3bj6e1vx2xo8-0>li{counter-increment:lst-ctn-kix_3bj6e1vx2xo8-0}ol.lst-kix_3bj6e1vx2xo8-3.start{counter-reset:lst-ctn-kix_3bj6e1vx2xo8-3 0}ol.lst-kix_3bj6e1vx2xo8-7.start{counter-reset:lst-ctn-kix_3bj6e1vx2xo8-7 0}.lst-kix_3bj6e1vx2xo8-2>li{counter-increment:lst-ctn-kix_3bj6e1vx2xo8-2}.lst-kix_3bj6e1vx2xo8-8>li:before{content:"" counter(lst-ctn-kix_3bj6e1vx2xo8-8,lower-roman) ". "}.lst-kix_3bj6e1vx2xo8-4>li:before{content:"" counter(lst-ctn-kix_3bj6e1vx2xo8-4,lower-latin) ". "}ol.lst-kix_3bj6e1vx2xo8-5.start{counter-reset:lst-ctn-kix_3bj6e1vx2xo8-5 0}.lst-kix_3bj6e1vx2xo8-8>li{counter-increment:lst-ctn-kix_3bj6e1vx2xo8-8}.lst-kix_ebc7j21k1a4o-5>li:before{content:"\0025a0  "}ol.lst-kix_3bj6e1vx2xo8-2.start{counter-reset:lst-ctn-kix_3bj6e1vx2xo8-2 0}.lst-kix_3bj6e1vx2xo8-7>li:before{content:"" counter(lst-ctn-kix_3bj6e1vx2xo8-7,lower-latin) ". "}.lst-kix_ebc7j21k1a4o-3>li:before{content:"\0025cf  "}.lst-kix_3bj6e1vx2xo8-6>li{counter-increment:lst-ctn-kix_3bj6e1vx2xo8-6}ol.lst-kix_3bj6e1vx2xo8-0.start{counter-reset:lst-ctn-kix_3bj6e1vx2xo8-0 0}.lst-kix_ebc7j21k1a4o-4>li:before{content:"\0025cb  "}.lst-kix_ebc7j21k1a4o-1>li:before{content:"\0025cb  "}.lst-kix_3bj6e1vx2xo8-6>li:before{content:"" counter(lst-ctn-kix_3bj6e1vx2xo8-6,decimal) ". "}.lst-kix_ebc7j21k1a4o-2>li:before{content:"\0025a0  "}.lst-kix_3bj6e1vx2xo8-4>li{counter-increment:lst-ctn-kix_3bj6e1vx2xo8-4}ol.lst-kix_3bj6e1vx2xo8-0{list-style-type:none}ol.lst-kix_3bj6e1vx2xo8-1{list-style-type:none}.lst-kix_3bj6e1vx2xo8-7>li{counter-increment:lst-ctn-kix_3bj6e1vx2xo8-7}ol.lst-kix_3bj6e1vx2xo8-2{list-style-type:none}ol.lst-kix_3bj6e1vx2xo8-3{list-style-type:none}ol.lst-kix_3bj6e1vx2xo8-4{list-style-type:none}ol.lst-kix_3bj6e1vx2xo8-5{list-style-type:none}.lst-kix_ebc7j21k1a4o-8>li:before{content:"\0025a0  "}ol.lst-kix_3bj6e1vx2xo8-6{list-style-type:none}ol.lst-kix_3bj6e1vx2xo8-7{list-style-type:none}ol.lst-kix_3bj6e1vx2xo8-8{list-style-type:none}ul.lst-kix_ebc7j21k1a4o-8{list-style-type:none}ul.lst-kix_ebc7j21k1a4o-7{list-style-type:none}ol.lst-kix_3bj6e1vx2xo8-8.start{counter-reset:lst-ctn-kix_3bj6e1vx2xo8-8 0}ul.lst-kix_ebc7j21k1a4o-6{list-style-type:none}ul.lst-kix_ebc7j21k1a4o-5{list-style-type:none}ul.lst-kix_ebc7j21k1a4o-4{list-style-type:none}ul.lst-kix_ebc7j21k1a4o-3{list-style-type:none}.lst-kix_3bj6e1vx2xo8-3>li{counter-increment:lst-ctn-kix_3bj6e1vx2xo8-3}ul.lst-kix_ebc7j21k1a4o-0{list-style-type:none}ul.lst-kix_ebc7j21k1a4o-1{list-style-type:none}.lst-kix_3bj6e1vx2xo8-5>li{counter-increment:lst-ctn-kix_3bj6e1vx2xo8-5}ul.lst-kix_ebc7j21k1a4o-2{list-style-type:none}ol.lst-kix_3bj6e1vx2xo8-6.start{counter-reset:lst-ctn-kix_3bj6e1vx2xo8-6 0}.lst-kix_3bj6e1vx2xo8-3>li:before{content:"" counter(lst-ctn-kix_3bj6e1vx2xo8-3,decimal) ". "}.lst-kix_ebc7j21k1a4o-7>li:before{content:"\0025cb  "}.lst-kix_3bj6e1vx2xo8-5>li:before{content:"" counter(lst-ctn-kix_3bj6e1vx2xo8-5,lower-roman) ". "}.lst-kix_ebc7j21k1a4o-0>li:before{content:"\0025cf  "}.lst-kix_3bj6e1vx2xo8-2>li:before{content:"" counter(lst-ctn-kix_3bj6e1vx2xo8-2,lower-roman) ". "}ol.lst-kix_3bj6e1vx2xo8-4.start{counter-reset:lst-ctn-kix_3bj6e1vx2xo8-4 0}ol{margin:0;padding:0}.c6{border-bottom-width:1pt;border-top-style:solid;width:66.9pt;border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-bottom-style:solid;vertical-align:top;border-top-color:#000000;border-left-color:#000000;border-right-color:#000000;border-left-style:solid;border-right-width:1pt;border-left-width:1pt}.c7{line-height:1.0;padding-top:0pt;text-align:left;direction:ltr;padding-bottom:0pt}.c4{vertical-align:baseline;color:#000000;font-style:normal;text-decoration:none;font-weight:normal}.c9{vertical-align:baseline;color:#000000;font-style:normal;text-decoration:none}.c16{margin-right:auto;border-collapse:collapse}.c3{font-size:6pt;font-family:"Times New Roman";font-weight:bold}.c15{color:#3d85c6;font-size:30pt;font-family:"Times New Roman"}.c0{widows:2;orphans:2;direction:ltr}.c1{font-size:16pt;font-family:"Times New Roman";font-weight:bold}.c2{font-size:12pt;font-family:"Times New Roman"}.c11{widows:2;orphans:2}.c21{font-size:16pt;font-family:"Times New Roman"}.c22{max-width:468pt;padding:72pt 72pt 72pt 72pt}.c18{font-size:6pt;font-family:"Times New Roman"}.c17{color:inherit;text-decoration:inherit}.c23{font-size:14pt;font-family:"Times New Roman"}.c19{color:#333333}.c13{background-color:#ffffff}.c8{font-weight:bold}.c10{text-decoration:underline}.c20{color:#ff0000}.c12{color:#1155cc}.c14{height:0pt}.c5{height:11pt}.title{widows:2;padding-top:0pt;line-height:1.15;orphans:2;text-align:left;color:#000000;font-size:21pt;font-family:"Trebuchet MS";padding-bottom:0pt;page-break-after:avoid}.subtitle{widows:2;padding-top:0pt;line-height:1.15;orphans:2;text-align:left;color:#666666;font-style:italic;font-size:13pt;font-family:"Trebuchet MS";padding-bottom:10pt;page-break-after:avoid}li{color:#000000;font-size:11pt;font-family:"Arial"}p{color:#000000;font-size:11pt;margin:0;font-family:"Arial"}h1{widows:2;padding-top:10pt;line-height:1.15;orphans:2;text-align:left;color:#000000;font-size:16pt;font-family:"Trebuchet MS";padding-bottom:0pt;page-break-after:avoid}h2{widows:2;padding-top:10pt;line-height:1.15;orphans:2;text-align:left;color:#000000;font-size:13pt;font-family:"Trebuchet MS";font-weight:bold;padding-bottom:0pt;page-break-after:avoid}h3{widows:2;padding-top:8pt;line-height:1.15;orphans:2;text-align:left;color:#666666;font-size:12pt;font-family:"Trebuchet MS";font-weight:bold;padding-bottom:0pt;page-break-after:avoid}h4{widows:2;padding-top:8pt;line-height:1.15;orphans:2;text-align:left;color:#666666;font-size:11pt;text-decoration:underline;font-family:"Trebuchet MS";padding-bottom:0pt;page-break-after:avoid}h5{widows:2;padding-top:8pt;line-height:1.15;orphans:2;text-align:left;color:#666666;font-size:11pt;font-family:"Trebuchet MS";padding-bottom:0pt;page-break-after:avoid}h6{widows:2;padding-top:8pt;line-height:1.15;orphans:2;text-align:left;color:#666666;font-style:italic;font-size:11pt;font-family:"Trebuchet MS";padding-bottom:0pt;page-break-after:avoid}</style></head><body class="c13 c22"><p class="c0"><span class="c15">Neural Nets</span></p><p class="c0"><span class="c2">CS 440 Spring 2015</span></p><p class="c0"><span class="c2">Programming Assignment 2</span></p><p class="c0"><span class="c2">Authors: Veena Dali (vdali@bu.edu), Leen AlShenibr (</span><span class="c2 c13">leenshe@bu.edu</span><span class="c2">)</span></p><p class="c0 c5"><span class="c1"></span></p><p class="c0"><span class="c1">Code</span><span class="c2 c8">&nbsp;</span></p><p class="c0"><span class="c2 c8 c10 c12"><a class="c17" href="https://www.google.com/url?q=https%3A%2F%2Fgithub.com%2FLeenAlShenibr%2FCS440_P2.git&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNGEpolvBMnMAWtPxDdxlXKGRHLXXw">https://github.com/LeenAlShenibr/CS440_P2.git</a></span></p><p class="c0 c5"><span class="c2 c8"></span></p><p class="c0"><span class="c1">Problem Definition</span></p><p class="c0 c5"><span class="c3"></span></p><p class="c0"><span class="c2">Our project aims to train a neural network to achieve its best performance using the back propagation algorithm. We are working to find the optimum parameters to help us achieve a good performance. The project&rsquo;s practical application to create a system that can learn and improve itself from data sets. </span></p><p class="c0"><span class="c2 c8">&nbsp;</span></p><p class="c0"><span class="c1">Method &amp; Implementation</span></p><p class="c0 c5"><span class="c3"></span></p><p class="c0"><span class="c2">The algorithm we used to train our neural network was the back propagation method. The algorithm consisted of computing the resulting outputs from the input data and then finding a beta for each node in the output layer by subtracting the desired output (found in the data given) by the computed output. These beta values were used for the next steps to calculate the beta for all other nodes in the network. </span><span class="c2">The weight changes were computed for each node by summing up all the weights for its connections to output nodes, and then adding it to the original node.</span><span class="c2">&nbsp;It runs the algorithm according to the training round parameter that we set. </span></p><p class="c0 c5"><span class="c2"></span></p><p class="c0"><span class="c2">We preprocessed the input data by normalizing it to improve the performance of the network. We used the skeleton code and implemented a training function with the algorithm described above. </span></p><p class="c0 c5"><span class="c2 c8"></span></p><p class="c0"><span class="c1">Experiment</span></p><p class="c0 c5"><span class="c3"></span></p><p class="c0"><span class="c2">Our experiment worked with three types of datasets: Boston University Biomedical Image Library (BUBIL), Lenses, and Credit Approval. </span><span class="c2">Prior to training, we had to preprocess the input data by normalizing it &nbsp;</span><span class="c2">using a guassian cumlative distribution function. Even though normalization is not necessary we have found that it yielded better results for our tests. Going from results that have a minimum error of 0.4 to 0.1 or less.</span></p><p class="c0 c5"><span class="c2"></span></p><p class="c0"><span class="c2">To test the performance of our network, we changed the following variables to find the best performance: performance rate, number of hidden layers, and rounds of training. When changing the parameters, we experimented with each dataset independently except for the rounds of training because we concluded that changing this parameter had the same effect on all datasets. To test the effects of rounds of training, we ran the tests with 100, 300, and 500 found the best results with the 300 training rounds for all datasets. We kept all other parameters for all data sets the same for consistency; below is a table outlining some of the results we got.</span></p><p class="c0 c5"><span class="c2"></span></p><p class="c0 c5"><span class="c2"></span></p><a href="#" name="643a72fcf047212f4f2f4789e23c39c803cfdb4e"></a><a href="#" name="0"></a><table cellpadding="0" cellspacing="0" class="c16"><tbody><tr class="c14"><td class="c6" colspan="1" rowspan="1"><p class="c7"><span class="c9 c2 c8">Num of training rounds</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c7"><span class="c9 c2 c8">Credit: training error</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c7"><span class="c9 c2 c8">Credit: testing error</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c7"><span class="c9 c2 c8">Lens: training error</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c7"><span class="c2 c8 c9">Lens: testing error</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c7 c11"><span class="c9 c2 c8">BUBIL: training error</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c7"><span class="c9 c2 c8">BUBIL: testing error</span></p></td></tr><tr class="c14"><td class="c6" colspan="1" rowspan="1"><p class="c7"><span class="c4 c2">500</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c7"><span class="c4 c2">0.200</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c7"><span class="c4 c2">0.357</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c7"><span class="c4 c2">0.0154</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c7"><span class="c4 c2">0.302</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c7 c11"><span class="c4 c2">0.309</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c7"><span class="c4 c2">0.272</span></p></td></tr><tr class="c14"><td class="c6" colspan="1" rowspan="1"><p class="c7"><span class="c4 c2">300</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c7"><span class="c4 c2">0.190</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c7"><span class="c4 c2">0.363</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c7"><span class="c4 c2">0.0247</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c7"><span class="c4 c2">0.304</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c7 c11"><span class="c4 c2">0.309</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c7"><span class="c4 c2">0.202</span></p></td></tr><tr class="c14"><td class="c6" colspan="1" rowspan="1"><p class="c7"><span class="c4 c2">100</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c7"><span class="c4 c2">0.204</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c7"><span class="c4 c2">0.329</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c7"><span class="c4 c2">0.0604</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c7"><span class="c4 c2">0.321</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c7"><span class="c2 c4">0.308</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c7"><span class="c4 c2">0.272</span></p></td></tr></tbody></table><p class="c0 c5"><span class="c2"></span></p><p class="c0"><span class="c2">The rest of the parameters were changed depending on the dataset. First, we changed the number of hidden layers and then the learning rate. While we changed the hidden layers, we keps the learning rate at 1. Once we found a good performance (low error), we experimented with the learning rate parameter while keeping the number of rounds and number of hidden layers the same as the ones we found for the optimum performances. For the number of hidden layers we tested the following values: 1, 10, 15, 30, 45. For the learning rate parameter we tested the following values: 0.5, 1, 1.5, 2, 3. So for each parameter we performed 5 tests.</span></p><p class="c0"><span class="c2 c8">&nbsp;</span></p><p class="c0"><span class="c1">Results</span></p><p class="c0 c5"><span class="c18"></span></p><p class="c0"><span class="c10 c23">Credit Approval</span></p><p class="c0"><span class="c2">We found a learning rate of 1.5 and 15 hidden layers to give us the lowest training error of 0.167. </span></p><p class="c0 c5"><span class="c2"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 290.00px; height: 195.00px;"><img alt="" src="images/image01.png" style="width: 290.00px; height: 587.37px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c5"><span class="c2 c10"></span></p><p class="c0"><span class="c2 c10">BUBIL</span></p><p class="c0"><span class="c2">We found a learning rate of 3.0 and 1 hidden layers to give us the lowest training error of 0.308. </span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 362.00px; height: 244.00px;"><img alt="" src="images/image00.png" style="width: 362.00px; height: 775.00px; margin-left: 0.00px; margin-top: -511.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c5"><span class="c2 c8"></span></p><p class="c0"><span class="c2 c10">Lenses</span></p><p class="c0"><span class="c2">We found a learning rate of 3.0 and 1 hidden layers to give us the lowest training error of 0.189. </span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 362.00px; height: 268.00px;"><img alt="" src="images/image00.png" style="width: 362.00px; height: 775.00px; margin-left: 0.00px; margin-top: -249.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c5"><span class="c2 c8"></span></p><p class="c0"><span class="c2 c8">&nbsp;</span></p><p class="c0"><span class="c1">Discussion</span></p><p class="c0 c5"><span class="c3"></span></p><p class="c0"><span class="c2">The strengths of using the backpropagation algorithm is that we can use it to train the neural net for virtually any type of data, the bigger the training set the better, according to our tests. This allows the algorithm to handle various types of datasets which is helpful in real-world application. Another strength of our system is that it is fast. Even when we increased our parameters significantly, it computed the error fast. </span></p><p class="c0 c5"><span class="c2"></span></p><p class="c0"><span class="c2">Some of the weaknesses with our method were for the BUBIL dataset. We found that the errors had a small difference when we changed the parameters. The BUBIL dataset is relatively small and it seems that the training algorithm does not work well with such small data sets. &nbsp;The algorithm can take a long time for large a large number of training inputs and training rounds, which can limit its use in some industries. One of the weakness of our method is that we are not using the error rate to adjust our learning rate. We have been doing this manually but if we had more time we would improve our method by incorporating this so that the system learns from the error rate. We could also include an algorithm or method that would tell us which parameters give us the best performance rather than what we did for this experiment which was comparing the training error values. &nbsp;</span></p><p class="c0"><span class="c2 c8">&nbsp;</span></p><p class="c0"><span class="c1">Conclusions</span></p><p class="c0 c5"><span class="c3"></span></p><p class="c0"><span class="c2">Depending on the size of the dataset, the parameters need to be changed accordingly. We found that datasets that are bigger like credit approval require more hidden layers. The learning rates 1.5 and 3 gave similar errors for BUBIL and lenses. We went with 3 because the testing error was better. Thus, we cannot conclude that the bigger datasets need smaller learning rates. </span><span class="c2">In the end, it&rsquo;s all up to experimentation with the values of hidden layers and learning rate that best fits the purposes of the data and its use. Our method successfully trains a neural network. </span></p><p class="c0"><span class="c2 c8">&nbsp;</span></p><p class="c0"><span class="c1">Credits and Bibliography:</span></p><p class="c0 c5"><span class="c3"></span></p><p class="c0"><span class="c2">Zhiqiang Ren&rsquo;s code was given as a skeleton code</span></p><p class="c0 c5"><span class="c18"></span></p><p class="c0"><span class="c2">McCaffrey, James. &ldquo;How To Standardize Data for Neural Networks.&rdquo; 15 Jan 2014. </span><span class="c2 c10 c12"><a class="c17" href="http://www.google.com/url?q=http%3A%2F%2Fvisualstudiomagazine.com%2Farticles%2F2014%2F01%2F01%2Fhow-to-standardize-data-for-neural-networks.aspx&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNEYPlTus1wthyU6soDQudC9321whA">Web</a></span><span class="c2">.</span></p><p class="c0 c5"><span class="c18"></span></p><p class="c0"><span class="c2">Collaborated with: Peter LaFontaine on parts of the back propagation algorithm.</span></p><p class="c0 c5"><span class="c2 c13 c19"></span></p></body></html>